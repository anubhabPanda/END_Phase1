{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "COQA_Dataset_Model2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dJke57UtfHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "770a9544-07c9-4a15-96c2-105ccd4893c8"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNbedn3dFD3B",
        "outputId": "93d82e5b-5dbb-4adb-a80e-c5d1213139b9"
      },
      "source": [
        "%cd /content/drive/MyDrive/END"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKHSCez4FT4v",
        "outputId": "bc8c3724-10df-4fab-8b43-00fedf426117"
      },
      "source": [
        "%%bash\r\n",
        "python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGax-MgGFW4x"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from pandas import Series, DataFrame\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import torchtext\r\n",
        "import spacy\r\n",
        "from torchtext.data import Field\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torchtext.data import BucketIterator\r\n",
        "import torch.optim as optim\r\n",
        "import time\r\n",
        "import random\r\n",
        "import math\r\n",
        "from torchtext import data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKu84enxG70D"
      },
      "source": [
        "nlp = spacy.load(\"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBuvP5UJJ_B6"
      },
      "source": [
        "def tokenize(text):\r\n",
        "  return [token.text for token in nlp.tokenizer(text)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKNAK_1qKGB6"
      },
      "source": [
        "# train_df.to_csv(\"COQA_train.csv\", index = False)\r\n",
        "train_df = pd.read_csv(\"COQA_train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe3VhtDnKMqz"
      },
      "source": [
        "text_field = Field(\r\n",
        "    sequential=True,\r\n",
        "    tokenize=tokenize, \r\n",
        "    lower=True,\r\n",
        "    init_token='<sos>', eos_token='<eos>'\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDMk3lqOKYeo"
      },
      "source": [
        "fields = {'ques_context' : ('ques_context', text_field),\r\n",
        "          'answer':('answer', text_field)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDDV7M_pKbO1"
      },
      "source": [
        "text_dataset = torchtext.data.TabularDataset(path='COQA_train.csv', format='CSV', fields=fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alE2pJB_Kdua"
      },
      "source": [
        "train, valid = text_dataset.split(split_ratio=[0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejSzvfXqKgnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c68fa25b-f8d3-4164-db94-06dbfaf8e72f"
      },
      "source": [
        "len(train), len(valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76053, 32594)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvSbsC0RKmir"
      },
      "source": [
        "MAX_VOCAB_SIZE = 20000\r\n",
        "text_field.build_vocab(train, min_freq=2, max_size=MAX_VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR9LPs3PKxeY"
      },
      "source": [
        "def get_example(data, example_number):\r\n",
        "    print(f\"Question and Context : \\n\\n{' '.join(data.examples[example_number].ques_context)}\")\r\n",
        "    print(f\"\\nAnswer : \\n\\n{' '.join(data.examples[example_number].answer)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZzMnpObK3j8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9ceea3-b41d-41d2-8fcd-6ab470c6186e"
      },
      "source": [
        "get_example(train, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question and Context : \n",
            "\n",
            "what made him a wartime leader ? bush 's margin of victory in the popular vote was the smallest ever for a reelected incumbent president , but marked the first time since his father 's victory 16 years prior that a candidate won a majority of the popular vote . the electoral map closely resembled that of 2000 , with only three states changing sides : new mexico and iowa voted republican in 2004 after having voted democratic in 2000 , while new hampshire voted democratic in 2004 after previously voting republican . in the electoral college , bush received 286 votes to kerry 's 252 . \n",
            "\n",
            " just eight months into his presidency , the terrorist attacks of september 11 , 2001 suddenly transformed bush into a wartime president . bush 's approval ratings surged to near 90 % . within a month , the forces of a coalition led by the united states entered afghanistan , which had been sheltering osama bin laden , suspected mastermind of the september 11 attacks . by december , the taliban had been removed as rulers of kabul , although a long and ongoing reconstruction would follow , severely hampered by ongoing turmoil and violence within the country .\n",
            "\n",
            "Answer : \n",
            "\n",
            "terrorists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8I9E-SbK4jt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d19cfda-92f5-40d3-9d3e-fe0fa132b77a"
      },
      "source": [
        "get_example(valid, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question and Context : \n",
            "\n",
            "is this the final ? ( cnn ) -- portsmouth will play chelsea in the fa cup final after an upset 2 - 0 extra - time victory over tottenham hotspur in the second semifinal at wembley on sunday . \n",
            "\n",
            " french striker frederic piquionne opened the scoring for avram grant 's men nine minutes into extra - time . \n",
            "\n",
            " former tottenham midfielder kevin - prince boateng scored the second with three minutes remaining from the penalty spot after referee alan wiley awarded a spot kick as wilson palacios fouled aruna dindane . \n",
            "\n",
            " it was a humiliating defeat for tottenham and their manager harry redknapp , who steered portsmouth to fa cup triumph in 2008 before leaving the cash - strapped club for white hart lane . \n",
            "\n",
            " his team went into the match as overwhelming favorites against a pompey team who had been relegated from the premier league the day before without playing , having been deducted nine points after going into administration . \n",
            "\n",
            " but all that was forgotten as their fanatical fans enjoyed a famous victory which owed much to good fortune and some excellent goalkeeping from england international david james . \n",
            "\n",
            " their breakthrough goal could be credited to the appalling wembley pitch as spurs defender michael dawson slipped at a crucial moment and piquionne took full advantage . \n",
            "\n",
            " tottenham thought they had equalized almost immediately through peter crouch but wiley ruled it out for a push on james . \n",
            "\n",
            " with tottenham camped in the pompey half , dindane broke clear and although palacios got the ball in his challenge he also pulled him down and wiley had no hesitation in pointing to the spot . \n",
            "\n",
            " spurs reject boateng scored past heurelho gomes with relish to seal a famous cup victory .\n",
            "\n",
            "Answer : \n",
            "\n",
            "yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQYfvDz_K7wv"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMlgLlR4K-Pt"
      },
      "source": [
        "BATCH_SIZE = 8\r\n",
        "\r\n",
        "train_iterator, valid_iterator = BucketIterator.splits((train, valid), batch_size = BATCH_SIZE, sort_key = lambda x: len(x.ques_context), sort_within_batch = False, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXhTydd9bEbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14af4ec9-d67b-4a84-b102-6d1c06875e9f"
      },
      "source": [
        "batch_ex = next(iter(train_iterator))\r\n",
        "batch_ex.ques_context"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
              "        [  50,  158,  106,  ...,   49,   55,   16],\n",
              "        [  16,   50,   14,  ...,   55,   15,   61],\n",
              "        ...,\n",
              "        [2158,    1,    1,  ...,    1,    1,    1],\n",
              "        [   6,    1,    1,  ...,    1,    1,    1],\n",
              "        [   3,    1,    1,  ...,    1,    1,    1]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX49ONCspn6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211d3775-ef75-473c-f908-7227c222a2af"
      },
      "source": [
        "batch_ex.ques_context.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([440, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxFcYgvNLE7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6dd951f-d96b-491d-f5ec-bb2ef76fad0d"
      },
      "source": [
        "[text_field.vocab.itos[x] for x in batch_ex.ques_context[:, 0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>',\n",
              " 'what',\n",
              " 'is',\n",
              " 'the',\n",
              " 'theory',\n",
              " 'of',\n",
              " 'why',\n",
              " 'children',\n",
              " 'like',\n",
              " 'santa',\n",
              " 'claus',\n",
              " 'so',\n",
              " 'much',\n",
              " '?',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'traditions',\n",
              " 'which',\n",
              " 'is',\n",
              " 'now',\n",
              " 'a',\n",
              " 'necessary',\n",
              " 'part',\n",
              " 'of',\n",
              " 'christmas',\n",
              " 'is',\n",
              " 'a',\n",
              " 'that',\n",
              " 'of',\n",
              " 'father',\n",
              " 'christmas',\n",
              " ',',\n",
              " 'or',\n",
              " 'santa',\n",
              " 'claus',\n",
              " '.',\n",
              " 'according',\n",
              " 'to',\n",
              " 'the',\n",
              " 'modern',\n",
              " 'legend',\n",
              " ',',\n",
              " 'he',\n",
              " 'is',\n",
              " 'a',\n",
              " 'magical',\n",
              " 'figure',\n",
              " 'who',\n",
              " 'visits',\n",
              " 'all',\n",
              " 'the',\n",
              " 'children',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " 'during',\n",
              " 'the',\n",
              " 'night',\n",
              " 'before',\n",
              " 'christmas',\n",
              " 'day',\n",
              " ',',\n",
              " 'leaving',\n",
              " 'presents',\n",
              " 'which',\n",
              " 'they',\n",
              " 'find',\n",
              " 'the',\n",
              " 'next',\n",
              " 'morning',\n",
              " '.',\n",
              " 'he',\n",
              " 'flies',\n",
              " 'through',\n",
              " 'the',\n",
              " 'night',\n",
              " 'sky',\n",
              " 'in',\n",
              " 'a',\n",
              " 'sledge',\n",
              " 'pulled',\n",
              " 'by',\n",
              " 'reindeer',\n",
              " ',',\n",
              " 'and',\n",
              " 'enters',\n",
              " 'houses',\n",
              " 'by',\n",
              " 'climbing',\n",
              " 'down',\n",
              " '<unk>',\n",
              " '.',\n",
              " 'this',\n",
              " 'strange',\n",
              " 'legend',\n",
              " 'is',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'life',\n",
              " 'of',\n",
              " 'a',\n",
              " 'man',\n",
              " 'called',\n",
              " 'nicholas',\n",
              " ',',\n",
              " 'but',\n",
              " 'in',\n",
              " 'fact',\n",
              " 'we',\n",
              " 'know',\n",
              " 'very',\n",
              " 'little',\n",
              " 'about',\n",
              " 'him',\n",
              " '.',\n",
              " 'historians',\n",
              " 'think',\n",
              " 'he',\n",
              " 'was',\n",
              " 'a',\n",
              " 'christian',\n",
              " 'bishop',\n",
              " 'in',\n",
              " 'turkey',\n",
              " 'in',\n",
              " 'about',\n",
              " '<unk>',\n",
              " '-',\n",
              " '<unk>',\n",
              " 'a.d.',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'stories',\n",
              " 'about',\n",
              " 'him',\n",
              " 'is',\n",
              " 'that',\n",
              " 'he',\n",
              " 'helped',\n",
              " 'three',\n",
              " 'poor',\n",
              " 'girls',\n",
              " '.',\n",
              " 'no',\n",
              " 'one',\n",
              " 'would',\n",
              " 'marry',\n",
              " 'them',\n",
              " 'because',\n",
              " 'they',\n",
              " 'were',\n",
              " 'so',\n",
              " 'poor',\n",
              " '.',\n",
              " 'to',\n",
              " 'provide',\n",
              " 'them',\n",
              " 'with',\n",
              " 'money',\n",
              " 'for',\n",
              " 'their',\n",
              " 'weddings',\n",
              " ',',\n",
              " 'nicholas',\n",
              " 'secretly',\n",
              " 'dropped',\n",
              " 'some',\n",
              " 'gold',\n",
              " 'coins',\n",
              " 'down',\n",
              " 'the',\n",
              " 'chimney',\n",
              " 'of',\n",
              " 'their',\n",
              " 'house',\n",
              " '.',\n",
              " 'after',\n",
              " 'nicholas',\n",
              " 'died',\n",
              " ',',\n",
              " 'he',\n",
              " 'was',\n",
              " 'made',\n",
              " 'a',\n",
              " 'saint',\n",
              " 'by',\n",
              " 'the',\n",
              " '<unk>',\n",
              " 'name',\n",
              " 'santa',\n",
              " 'claus',\n",
              " 'thus',\n",
              " 'comes',\n",
              " 'from',\n",
              " 'st',\n",
              " 'nicholas',\n",
              " '.',\n",
              " ')',\n",
              " 'his',\n",
              " 'feast',\n",
              " 'day',\n",
              " 'was',\n",
              " 'celebrated',\n",
              " 'in',\n",
              " 'december',\n",
              " ',',\n",
              " 'and',\n",
              " 'parents',\n",
              " 'started',\n",
              " 'giving',\n",
              " 'their',\n",
              " 'children',\n",
              " 'secret',\n",
              " 'presents',\n",
              " 'from',\n",
              " 'st',\n",
              " 'nicholas',\n",
              " '.',\n",
              " 'over',\n",
              " 'the',\n",
              " 'years',\n",
              " ',',\n",
              " 'this',\n",
              " 'custom',\n",
              " 'became',\n",
              " 'part',\n",
              " 'of',\n",
              " 'our',\n",
              " 'christmas',\n",
              " 'traditions',\n",
              " '.',\n",
              " '\\n\\n',\n",
              " 'recently',\n",
              " ',',\n",
              " 'a',\n",
              " 'psychologist',\n",
              " 'has',\n",
              " 'claimed',\n",
              " 'that',\n",
              " 'father',\n",
              " 'christmas',\n",
              " 'is',\n",
              " '\"',\n",
              " 'the',\n",
              " 'perfect',\n",
              " 'fantasy',\n",
              " '\"',\n",
              " 'for',\n",
              " 'children',\n",
              " '.',\n",
              " 'according',\n",
              " 'to',\n",
              " 'professor',\n",
              " 'anthony',\n",
              " 'clare',\n",
              " ',',\n",
              " 'children',\n",
              " 'love',\n",
              " 'the',\n",
              " 'character',\n",
              " 'of',\n",
              " 'father',\n",
              " 'christmas',\n",
              " 'because',\n",
              " 'he',\n",
              " 'is',\n",
              " 'like',\n",
              " 'an',\n",
              " 'ideal',\n",
              " 'father',\n",
              " ':',\n",
              " 'he',\n",
              " 'loves',\n",
              " 'children',\n",
              " 'and',\n",
              " 'gives',\n",
              " 'them',\n",
              " 'presents',\n",
              " ',',\n",
              " 'but',\n",
              " 'he',\n",
              " 'never',\n",
              " '<unk>',\n",
              " 'them',\n",
              " ',',\n",
              " 'is',\n",
              " 'never',\n",
              " 'angry',\n",
              " ',',\n",
              " 'and',\n",
              " 'children',\n",
              " 'do',\n",
              " 'not',\n",
              " 'even',\n",
              " 'need',\n",
              " 'to',\n",
              " 'thank',\n",
              " 'him',\n",
              " 'for',\n",
              " 'the',\n",
              " 'presents',\n",
              " '.',\n",
              " 'other',\n",
              " 'writers',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'point',\n",
              " 'out',\n",
              " 'that',\n",
              " 'father',\n",
              " 'christmas',\n",
              " 'can',\n",
              " 'be',\n",
              " 'a',\n",
              " 'frightening',\n",
              " 'character',\n",
              " 'to',\n",
              " 'some',\n",
              " 'children',\n",
              " '.',\n",
              " 'jane',\n",
              " '<unk>',\n",
              " 'says',\n",
              " 'that',\n",
              " 'some',\n",
              " 'children',\n",
              " 'are',\n",
              " 'terrified',\n",
              " 'of',\n",
              " 'this',\n",
              " 'fat',\n",
              " ',',\n",
              " 'bearded',\n",
              " 'old',\n",
              " 'man',\n",
              " '.',\n",
              " 'it',\n",
              " 'can',\n",
              " 'certainly',\n",
              " 'confuse',\n",
              " 'many',\n",
              " 'children',\n",
              " '.',\n",
              " 'as',\n",
              " 'parents',\n",
              " ',',\n",
              " 'we',\n",
              " 'warn',\n",
              " 'our',\n",
              " 'children',\n",
              " 'to',\n",
              " 'be',\n",
              " 'careful',\n",
              " 'of',\n",
              " 'strangers',\n",
              " 'and',\n",
              " 'never',\n",
              " 'to',\n",
              " 'let',\n",
              " 'them',\n",
              " 'into',\n",
              " 'the',\n",
              " 'house',\n",
              " ',',\n",
              " 'and',\n",
              " 'yet',\n",
              " 'we',\n",
              " 'tell',\n",
              " 'children',\n",
              " 'that',\n",
              " 'a',\n",
              " 'strange',\n",
              " 'man',\n",
              " 'will',\n",
              " 'come',\n",
              " 'into',\n",
              " 'their',\n",
              " 'bedroom',\n",
              " 'at',\n",
              " 'night',\n",
              " '!',\n",
              " 'some',\n",
              " 'children',\n",
              " 'can',\n",
              " 'become',\n",
              " 'very',\n",
              " 'worried',\n",
              " 'about',\n",
              " 'this',\n",
              " 'idea',\n",
              " 'and',\n",
              " 'fear',\n",
              " 'that',\n",
              " 'he',\n",
              " 'is',\n",
              " 'a',\n",
              " 'kind',\n",
              " 'of',\n",
              " 'burglar',\n",
              " '.',\n",
              " '\\n\\n',\n",
              " 'most',\n",
              " 'children',\n",
              " ',',\n",
              " 'however',\n",
              " ',',\n",
              " 'understand',\n",
              " 'from',\n",
              " 'their',\n",
              " 'parents',\n",
              " 'and',\n",
              " 'from',\n",
              " 'the',\n",
              " 'media',\n",
              " 'that',\n",
              " 'father',\n",
              " 'christmas',\n",
              " 'is',\n",
              " 'basically',\n",
              " 'a',\n",
              " '_',\n",
              " 'character',\n",
              " ',',\n",
              " 'and',\n",
              " 'look',\n",
              " 'forward',\n",
              " 'to',\n",
              " 'his',\n",
              " 'annual',\n",
              " 'visit',\n",
              " 'with',\n",
              " 'joy',\n",
              " 'and',\n",
              " 'excitement',\n",
              " '.',\n",
              " '<eos>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbN8k12dQ2g7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9e5389b-ef72-4ca3-cc86-9df5ebabbf82"
      },
      "source": [
        "[text_field.vocab.itos[x] for x in batch_ex.answer[:, 0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<sos>', 'he', 'is', 'like', 'an', 'ideal', 'father', '<eos>', '<pad>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1Jsck4jLHCg"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(src))\r\n",
        "        \r\n",
        "        #embedded = [src len, batch size, emb dim]\r\n",
        "        \r\n",
        "        outputs, hidden = self.rnn(embedded)\r\n",
        "                \r\n",
        "        #outputs = [src len, batch size, hid dim * num directions]\r\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\r\n",
        "        #outputs are always from the last layer\r\n",
        "        \r\n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \r\n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\r\n",
        "        \r\n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \r\n",
        "        #  encoder RNNs fed through a linear layer\r\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\r\n",
        "        \r\n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        \r\n",
        "        return outputs, hidden\r\n",
        "\r\n",
        "class Attention(nn.Module):\r\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\r\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\r\n",
        "        \r\n",
        "    def forward(self, hidden, encoder_outputs):\r\n",
        "        \r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        batch_size = encoder_outputs.shape[1]\r\n",
        "        src_len = encoder_outputs.shape[0]\r\n",
        "        \r\n",
        "        #repeat decoder hidden state src_len times\r\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\r\n",
        "        \r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #hidden = [batch size, src len, dec hid dim]\r\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n",
        "        \r\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \r\n",
        "        \r\n",
        "        #energy = [batch size, src len, dec hid dim]\r\n",
        "\r\n",
        "        attention = self.v(energy).squeeze(2)\r\n",
        "        \r\n",
        "        #attention= [batch size, src len]\r\n",
        "        \r\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6oGuBatLN_H"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.output_dim = output_dim\r\n",
        "        self.attention = attention\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, input, hidden, encoder_outputs):\r\n",
        "             \r\n",
        "        #input = [batch size]\r\n",
        "        #hidden = [batch size, dec hid dim]\r\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        input = input.unsqueeze(0)\r\n",
        "        \r\n",
        "        #input = [1, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(input))\r\n",
        "        \r\n",
        "        #embedded = [1, batch size, emb dim]\r\n",
        "        \r\n",
        "        a = self.attention(hidden, encoder_outputs)\r\n",
        "                \r\n",
        "        #a = [batch size, src len]\r\n",
        "        \r\n",
        "        a = a.unsqueeze(1)\r\n",
        "        \r\n",
        "        #a = [batch size, 1, src len]\r\n",
        "        \r\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\r\n",
        "        \r\n",
        "        weighted = torch.bmm(a, encoder_outputs)\r\n",
        "        \r\n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\r\n",
        "        \r\n",
        "        weighted = weighted.permute(1, 0, 2)\r\n",
        "        \r\n",
        "        #weighted = [1, batch size, enc hid dim * 2]\r\n",
        "        \r\n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\r\n",
        "        \r\n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\r\n",
        "            \r\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\r\n",
        "        \r\n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\r\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\r\n",
        "        \r\n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\r\n",
        "        #output = [1, batch size, dec hid dim]\r\n",
        "        #hidden = [1, batch size, dec hid dim]\r\n",
        "        #this also means that output == hidden\r\n",
        "        assert (output == hidden).all()\r\n",
        "        \r\n",
        "        embedded = embedded.squeeze(0)\r\n",
        "        output = output.squeeze(0)\r\n",
        "        weighted = weighted.squeeze(0)\r\n",
        "        \r\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\r\n",
        "        \r\n",
        "        #prediction = [batch size, output dim]\r\n",
        "        \r\n",
        "        return prediction, hidden.squeeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5Oc5Yh5LQ8z"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\r\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\r\n",
        "        \r\n",
        "        batch_size = src.shape[1]\r\n",
        "        trg_len = trg.shape[0]\r\n",
        "        trg_vocab_size = self.decoder.output_dim\r\n",
        "        \r\n",
        "        #tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\r\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\r\n",
        "        encoder_outputs, hidden = self.encoder(src)\r\n",
        "                \r\n",
        "        #first input to the decoder is the <sos> tokens\r\n",
        "        input = trg[0,:]\r\n",
        "        \r\n",
        "        for t in range(1, trg_len):\r\n",
        "            \r\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\r\n",
        "            #receive output tensor (predictions) and new hidden state\r\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\r\n",
        "            \r\n",
        "            #place predictions in a tensor holding predictions for each token\r\n",
        "            outputs[t] = output\r\n",
        "            \r\n",
        "            #decide if we are going to use teacher forcing or not\r\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\r\n",
        "            \r\n",
        "            #get the highest predicted token from our predictions\r\n",
        "            top1 = output.argmax(1) \r\n",
        "            \r\n",
        "            #if teacher forcing, use actual next token as next input\r\n",
        "            #if not, use predicted token\r\n",
        "            input = trg[t] if teacher_force else top1\r\n",
        "\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h14I90Y0LTo1"
      },
      "source": [
        "INPUT_DIM = len(text_field.vocab)\r\n",
        "OUTPUT_DIM = len(text_field.vocab)\r\n",
        "ENC_EMB_DIM = 128\r\n",
        "DEC_EMB_DIM = 128\r\n",
        "ENC_HID_DIM = 128\r\n",
        "DEC_HID_DIM = 128\r\n",
        "ENC_DROPOUT = 0.5\r\n",
        "DEC_DROPOUT = 0.5\r\n",
        "\r\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\r\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\r\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YlxCknJLWs7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ac862a-1ddc-44a6-8a75-c3c338d6203b"
      },
      "source": [
        "def init_weights(m):\r\n",
        "    for name, param in m.named_parameters():\r\n",
        "        if 'weight' in name:\r\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
        "        else:\r\n",
        "            nn.init.constant_(param.data, 0)\r\n",
        "            \r\n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(20004, 128)\n",
              "    (rnn): GRU(128, 128, bidirectional=True)\n",
              "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
              "    )\n",
              "    (embedding): Embedding(20004, 128)\n",
              "    (rnn): GRU(384, 128)\n",
              "    (fc_out): Linear(in_features=512, out_features=20004, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiUUk_4ILYqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d294ad1-d24f-4cdc-d4a8-e24fbaa5a84c"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 15,860,900 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhTnF_SbLbiD"
      },
      "source": [
        "optimizer = optim.AdamW(model.parameters())\r\n",
        "PAD_IDX = text_field.vocab.stoi[text_field.pad_token]\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRck8b8ZLi4z"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.ques_context\r\n",
        "        trg = batch.answer\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output = model(src, trg)\r\n",
        "        \r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #output = [trg len, batch size, output dim]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output[1:].view(-1, output_dim)\r\n",
        "        trg = trg[1:].view(-1)\r\n",
        "        \r\n",
        "        #trg = [(trg len - 1) * batch size]\r\n",
        "        #output = [(trg len - 1) * batch size, output dim]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Fxb_JtLjaH"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.ques_context\r\n",
        "            trg = batch.answer\r\n",
        "\r\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\r\n",
        "\r\n",
        "            #trg = [trg len, batch size]\r\n",
        "            #output = [trg len, batch size, output dim]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output[1:].view(-1, output_dim)\r\n",
        "            trg = trg[1:].view(-1)\r\n",
        "\r\n",
        "            #trg = [(trg len - 1) * batch size]\r\n",
        "            #output = [(trg len - 1) * batch size, output dim]\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTeZRHwLmJS"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq4i8vPvLpEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b6d894-db0f-4928-f97b-2fc25ab06fc0"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'COQA_model2.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 16m 14s\n",
            "\tTrain Loss: 5.123 | Train PPL: 167.800\n",
            "\t Val. Loss: 4.975 |  Val. PPL: 144.808\n",
            "Epoch: 02 | Time: 16m 21s\n",
            "\tTrain Loss: 4.584 | Train PPL:  97.891\n",
            "\t Val. Loss: 4.866 |  Val. PPL: 129.780\n",
            "Epoch: 03 | Time: 16m 32s\n",
            "\tTrain Loss: 4.324 | Train PPL:  75.457\n",
            "\t Val. Loss: 4.819 |  Val. PPL: 123.795\n",
            "Epoch: 04 | Time: 16m 23s\n",
            "\tTrain Loss: 4.098 | Train PPL:  60.220\n",
            "\t Val. Loss: 4.816 |  Val. PPL: 123.483\n",
            "Epoch: 05 | Time: 16m 32s\n",
            "\tTrain Loss: 3.909 | Train PPL:  49.847\n",
            "\t Val. Loss: 4.807 |  Val. PPL: 122.373\n",
            "Epoch: 06 | Time: 16m 25s\n",
            "\tTrain Loss: 3.741 | Train PPL:  42.150\n",
            "\t Val. Loss: 4.840 |  Val. PPL: 126.519\n",
            "Epoch: 07 | Time: 16m 16s\n",
            "\tTrain Loss: 3.602 | Train PPL:  36.656\n",
            "\t Val. Loss: 4.870 |  Val. PPL: 130.376\n",
            "Epoch: 08 | Time: 16m 17s\n",
            "\tTrain Loss: 3.488 | Train PPL:  32.722\n",
            "\t Val. Loss: 4.874 |  Val. PPL: 130.780\n",
            "Epoch: 09 | Time: 16m 13s\n",
            "\tTrain Loss: 3.385 | Train PPL:  29.504\n",
            "\t Val. Loss: 4.910 |  Val. PPL: 135.699\n",
            "Epoch: 10 | Time: 16m 19s\n",
            "\tTrain Loss: 3.303 | Train PPL:  27.193\n",
            "\t Val. Loss: 4.931 |  Val. PPL: 138.538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojY0hOdaLxYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa110ad3-b6b8-405c-9338-2e38673469ad"
      },
      "source": [
        "model.load_state_dict(torch.load('COQA_model2.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zf296GzcZ8U"
      },
      "source": [
        "batch = next(iter(valid_iterator))\r\n",
        "src = batch.ques_context\r\n",
        "trg = batch.answer\r\n",
        "prediction = model(src, trg, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FwqfqwGbfpG",
        "outputId": "fbe5bc0b-c498-4b78-f766-37d45159b60a"
      },
      "source": [
        "prediction[1:].view(-1, len(text_field.vocab)).cpu().shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([56, 20004])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yTmeXjscVA_",
        "outputId": "2d5e468b-c3a1-4259-fc72-088d33da8fec"
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 8, 20004])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3KKn6E_e_0h"
      },
      "source": [
        "import textwrap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI-zFuKadmqb"
      },
      "source": [
        "def show_predictions(src, trg, example_id):\r\n",
        "\r\n",
        "  print(\"Question and Context:\\n\")\r\n",
        "  for e in textwrap.wrap(\" \".join([text_field.vocab.itos[x] for x in src[:, example_id].cpu()]), 100):\r\n",
        "    print(e)\r\n",
        "\r\n",
        "  print(\"\\nActual Answer: \\n\")\r\n",
        "  print(\" \".join([text_field.vocab.itos[x] for x in trg[:, example_id].cpu()]))\r\n",
        "\r\n",
        "  print(\"\\nPredicted Answer: \\n\")\r\n",
        "  print(\" \".join([text_field.vocab.itos[x] for x in torch.argmax(prediction[:, example_id, :], axis = 1).cpu()]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1N15L2LeCUC",
        "outputId": "b462382b-347b-4d81-aa99-660b71928e30"
      },
      "source": [
        "show_predictions(src, trg, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question and Context:\n",
            "\n",
            "<sos> what did he like to collect the most ? timmy liked to play games and play sports but more than\n",
            "anything he liked to collect things . he collected bottle caps . he collected sea shells . he\n",
            "collected baseball cards . he has collected baseball cards the longest . he likes to collect the\n",
            "thing that he has collected the longest the most . he once thought about collecting stamps but never\n",
            "did . his most expensive collection was not his favorite collection . timmy spent the most money on\n",
            "his bottle cap collection . <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Actual Answer: \n",
            "\n",
            "<sos> baseball cards <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Predicted Answer: \n",
            "\n",
            "<unk> his own <eos> <eos> <eos> <eos> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9Mi02oIgp7g",
        "outputId": "ab9897ae-3e0e-4cd5-d810-362d3cad90f3"
      },
      "source": [
        "show_predictions(src, trg, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question and Context:\n",
            "\n",
            "<sos> was his baseball card collection the most expensive ? timmy liked to play games and play\n",
            "sports but more than anything he liked to collect things . he collected bottle caps . he collected\n",
            "sea shells . he collected baseball cards . he has collected baseball cards the longest . he likes to\n",
            "collect the thing that he has collected the longest the most . he once thought about collecting\n",
            "stamps but never did . his most expensive collection was not his favorite collection . timmy spent\n",
            "the most money on his bottle cap collection . <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Actual Answer: \n",
            "\n",
            "<sos> no <eos> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Predicted Answer: \n",
            "\n",
            "<unk> no <eos> <eos> <eos> <eos> <eos> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bdi2QRSgs8X",
        "outputId": "4162da2a-a92a-4d24-ce07-14a0e5da3873"
      },
      "source": [
        "show_predictions(src, trg, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question and Context:\n",
            "\n",
            "<sos> does she have any pets with feathers ? molly likes animals . she has a cat . she has a dog .\n",
            "she has a bird . she has a hamster . she has a bunny . her cat 's name is kitty . her dog 's name is\n",
            "spike . her bird 's name is polly . her hamster 's name is barry . her bunny 's name is snowball .\n",
            "kitty plays with yarn . spike plays with a ball . polly plays in her cage . barry runs on his wheel\n",
            ". snowball eats carrots . <eos> <pad>\n",
            "\n",
            "Actual Answer: \n",
            "\n",
            "<sos> yes <eos> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Predicted Answer: \n",
            "\n",
            "<unk> yes <eos> <eos> <eos> <eos> <eos> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuLLGGXohbsB",
        "outputId": "0514f9f9-09ea-4945-bf0a-fb10b444a12f"
      },
      "source": [
        "show_predictions(src, trg, 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question and Context:\n",
            "\n",
            "<sos> what does snowball like ? molly likes animals . she has a cat . she has a dog . she has a bird\n",
            ". she has a hamster . she has a bunny . her cat 's name is kitty . her dog 's name is spike . her\n",
            "bird 's name is polly . her hamster 's name is barry . her bunny 's name is snowball . kitty plays\n",
            "with yarn . spike plays with a ball . polly plays in her cage . barry runs on his wheel . snowball\n",
            "eats carrots . <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Actual Answer: \n",
            "\n",
            "<sos> a bunny <eos> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Predicted Answer: \n",
            "\n",
            "<unk> a ball <eos> ball <eos> <eos> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGz_Jp5ehKh2"
      },
      "source": [
        "#### The model seems to be doing well in Yes/No Questions but is failing in other descriptive questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd36H8d4hBaa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}