{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OpenBookQA_Model1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSY2axW_9CGG",
        "outputId": "1be67338-6ab6-4705-9b14-004df321f93e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hkakgML9FPN",
        "outputId": "55ad788e-53b6-4c8f-8e2d-f568b9ce8d6a"
      },
      "source": [
        "%cd /content/drive/MyDrive/END"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/END\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc01fiBXN6y_"
      },
      "source": [
        "**Getting the dataset and saving in drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j99dcTB9tKg",
        "outputId": "380193cc-72f7-4e80-e119-7ea4af7ceac0"
      },
      "source": [
        "# !wget https://s3-us-west-2.amazonaws.com/ai2-website/data/OpenBookQA-V1-Sep2018.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-11 10:26:52--  https://s3-us-west-2.amazonaws.com/ai2-website/data/OpenBookQA-V1-Sep2018.zip\n",
            "Resolving s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)... 52.218.221.72\n",
            "Connecting to s3-us-west-2.amazonaws.com (s3-us-west-2.amazonaws.com)|52.218.221.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1446098 (1.4M) [application/zip]\n",
            "Saving to: ‘OpenBookQA-V1-Sep2018.zip’\n",
            "\n",
            "OpenBookQA-V1-Sep20 100%[===================>]   1.38M  1.44MB/s    in 1.0s    \n",
            "\n",
            "2021-01-11 10:26:54 (1.44 MB/s) - ‘OpenBookQA-V1-Sep2018.zip’ saved [1446098/1446098]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocr3FSUt-DoD",
        "outputId": "324b1548-565e-4da9-d7a5-1ca7e54a799f"
      },
      "source": [
        "# !unzip /content/drive/MyDrive/END/OpenBookQA-V1-Sep2018.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/END/OpenBookQA-V1-Sep2018.zip\n",
            "   creating: OpenBookQA-V1-Sep2018/\n",
            "   creating: OpenBookQA-V1-Sep2018/Data/\n",
            "   creating: OpenBookQA-V1-Sep2018/Data/Additional/\n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/test_complete.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/train_complete.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/crowdsourced-facts.txt  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Additional/dev_complete.jsonl  \n",
            "   creating: OpenBookQA-V1-Sep2018/Data/Main/\n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/train.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/test.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/train.tsv  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/dev.tsv  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/openbook.txt  \n",
            "  inflating: OpenBookQA-V1-Sep2018/Data/Main/test.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3_kswMd_yIG"
      },
      "source": [
        "# !rm -r /content/drive/MyDrive/END/OpenBookQA-V1-Sep2018.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TurrCuR3N1uh"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs-kOGD0_LSe"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from pandas import Series, DataFrame\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import torchtext\r\n",
        "import spacy\r\n",
        "from spacy.lang.zh import Chinese\r\n",
        "from torchtext.data import Field\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchtext.data import BucketIterator\r\n",
        "import torch.optim as optim\r\n",
        "import time\r\n",
        "import random\r\n",
        "import math\r\n",
        "from torchtext import data as data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjigOzS8Hq1X"
      },
      "source": [
        "SEED = 1234\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kULpJKv3_wLY",
        "outputId": "c6dea8ed-1198-4a56-f851-885bfa1ea654"
      },
      "source": [
        "%%bash\r\n",
        "python -m spacy download en"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.1.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wASaZxkcNfsc"
      },
      "source": [
        "**Let's create a function creat_dataframe() to convert the json file to a dataframe for ease of use**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVsuB_1EDzav"
      },
      "source": [
        "def retrieve_answer(row):\r\n",
        "  correct_answer = row['answerKey']\r\n",
        "  choices = row['answer_choice']\r\n",
        "  return [x['text'] for x in choices if x['label'] == correct_answer][0]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3ZessfsE71_"
      },
      "source": [
        "def create_dataframe(data):\r\n",
        "  data['question_stem'] = data['question'].apply(lambda x:x['stem'])\r\n",
        "  data['answer_choice'] = data['question'].apply(lambda x:x['choices'])\r\n",
        "  data.drop('question', axis=1, inplace = True)\r\n",
        "  data['answer'] = data.apply(lambda row:retrieve_answer(row), axis = 1)\r\n",
        "  data.rename(columns = {'question_stem':'question'}, inplace = True)\r\n",
        "  cols_to_return = ['question', 'answer']\r\n",
        "  return data[cols_to_return]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjcmIeyqFxPI"
      },
      "source": [
        "train_df = pd.read_json('/content/drive/MyDrive/END/OpenBookQA-V1-Sep2018/Data/Main/train.jsonl', lines = True)\r\n",
        "valid_df = pd.read_json('/content/drive/MyDrive/END/OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl', lines = True)\r\n",
        "test_df = pd.read_json('/content/drive/MyDrive/END/OpenBookQA-V1-Sep2018/Data/Main/test.jsonl', lines = True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "I-Mwb9ifGAPK",
        "outputId": "d0d2bb9b-7ef4-4b11-ca67-908c415bec94"
      },
      "source": [
        "create_dataframe(train_df.copy())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sun is responsible for</td>\n",
              "      <td>plants sprouting, blooming and wilting</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When standing miles away from Mount Rushmore</td>\n",
              "      <td>the mountains seem smaller than in photographs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When food is reduced in the stomach</td>\n",
              "      <td>nutrients are being deconstructed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stars are</td>\n",
              "      <td>great balls of gas burning billions of miles away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You can make a telescope with a</td>\n",
              "      <td>mailing tube</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4952</th>\n",
              "      <td>A bulldozer alters the area of</td>\n",
              "      <td>skyscrapers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4953</th>\n",
              "      <td>An organism that can survive without the help ...</td>\n",
              "      <td>Brewer's yeast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4954</th>\n",
              "      <td>The nimbleness of this animal is a key adaptio...</td>\n",
              "      <td>the antelope</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4955</th>\n",
              "      <td>Birds will have different kinds of beaks depen...</td>\n",
              "      <td>organisms they hunt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4956</th>\n",
              "      <td>Harriet wants to know the area of a rectangula...</td>\n",
              "      <td>a ruler</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4957 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               question                                             answer\n",
              "0                            The sun is responsible for             plants sprouting, blooming and wilting\n",
              "1          When standing miles away from Mount Rushmore     the mountains seem smaller than in photographs\n",
              "2                   When food is reduced in the stomach                  nutrients are being deconstructed\n",
              "3                                             Stars are  great balls of gas burning billions of miles away\n",
              "4                       You can make a telescope with a                                       mailing tube\n",
              "...                                                 ...                                                ...\n",
              "4952                     A bulldozer alters the area of                                        skyscrapers\n",
              "4953  An organism that can survive without the help ...                                     Brewer's yeast\n",
              "4954  The nimbleness of this animal is a key adaptio...                                       the antelope\n",
              "4955  Birds will have different kinds of beaks depen...                                organisms they hunt\n",
              "4956  Harriet wants to know the area of a rectangula...                                            a ruler\n",
              "\n",
              "[4957 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IAbMmp5GjBv"
      },
      "source": [
        "train_df = create_dataframe(train_df.copy())\r\n",
        "valid_df = create_dataframe(valid_df.copy())\r\n",
        "test_df = create_dataframe(test_df.copy())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmwFAryrGtTl",
        "outputId": "8cf8bdf9-35c7-4127-d4fb-150a903c26db"
      },
      "source": [
        "train_df.shape, valid_df.shape, test_df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4957, 2), (500, 2), (500, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKy688WVG_s0",
        "outputId": "b88871bf-b91b-4d26-edc5-bec638b5e77e"
      },
      "source": [
        "print(\"Train Data:\")\r\n",
        "print(train_df.head())\r\n",
        "\r\n",
        "print(\"\\nValid Data:\")\r\n",
        "print(valid_df.head())\r\n",
        "\r\n",
        "print(\"\\nTest Data:\")\r\n",
        "print(test_df.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data:\n",
            "                                       question                                             answer\n",
            "0                    The sun is responsible for             plants sprouting, blooming and wilting\n",
            "1  When standing miles away from Mount Rushmore     the mountains seem smaller than in photographs\n",
            "2           When food is reduced in the stomach                  nutrients are being deconstructed\n",
            "3                                     Stars are  great balls of gas burning billions of miles away\n",
            "4               You can make a telescope with a                                       mailing tube\n",
            "\n",
            "Valid Data:\n",
            "                                            question                          answer\n",
            "0  Frilled sharks and angler fish live far beneat...                Deep sea animals\n",
            "1  Gas can fill any container it is given, and li...              uses what it needs\n",
            "2  When birds migrate south for the winter, they ...  they are genetically called to\n",
            "3  If a person walks in the opposite direction of...                           south\n",
            "4         An example of lots kinetic energy would be       An aircraft taking a trip\n",
            "\n",
            "Test Data:\n",
            "                                            question                        answer\n",
            "0  A person wants to start saving money so that t...         quit eating lunch out\n",
            "1       There is most likely going to be fog around:                       a marsh\n",
            "2                                      Predators eat                       bunnies\n",
            "3  Oak tree seeds are planted and a sidewalk is p...  parts may break the concrete\n",
            "4            An electric car runs on electricity via         electrical conductors\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R12pGP3bNtoq"
      },
      "source": [
        "**Load the english language model from spacy and create tokenize function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJmtK6BGHEro"
      },
      "source": [
        "spacy_en = spacy.load('en')\r\n",
        "def tokenize(text):\r\n",
        "  return [token.text for token in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbNrwYQ8M7gp"
      },
      "source": [
        "**Since we wan't the Question and Answer to have the same vocabulary, we will create a single field object called text field for both question and answer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muCfzjfLH2XP"
      },
      "source": [
        "text_field = Field(\r\n",
        "    sequential=True,\r\n",
        "    tokenize=tokenize, \r\n",
        "    lower=True,\r\n",
        "    init_token='<sos>', eos_token='<eos>'\r\n",
        ")\r\n",
        "\r\n",
        "fields = [('question', text_field),('answer', text_field)]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KU0d_2LIqqi"
      },
      "source": [
        "example_train = [data.Example.fromlist([train_df.question[i],train_df.answer[i]], fields) for i in range(train_df.shape[0])] \r\n",
        "example_valid = [data.Example.fromlist([valid_df.question[i],valid_df.answer[i]], fields) for i in range(valid_df.shape[0])] \r\n",
        "example_test = [data.Example.fromlist([test_df.question[i],test_df.answer[i]], fields) for i in range(test_df.shape[0])]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vFdn7SUJAE4"
      },
      "source": [
        "# Creating datasets\r\n",
        "train_data = data.Dataset(example_train, fields)\r\n",
        "valid_data = data.Dataset(example_valid, fields)\r\n",
        "test_data = data.Dataset(example_test, fields)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FYhgYUVNL6o"
      },
      "source": [
        "**Let's look at few examples from the train, valid and test datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pPUKk9SJkI9"
      },
      "source": [
        "def get_example(data, example_number):\r\n",
        "    print(f\"Question : \\n\\n{' '.join(data.examples[example_number].question)}\")\r\n",
        "    print(f\"\\nAnswer : \\n\\n{' '.join(data.examples[example_number].answer)}\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtlHxh2gJlh4",
        "outputId": "7048cb79-038d-4c3a-fcf3-024b78b6923d"
      },
      "source": [
        "get_example(train_data, 0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question : \n",
            "\n",
            "the sun is responsible for\n",
            "\n",
            "Answer : \n",
            "\n",
            "plants sprouting , blooming and wilting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWUtg6h6J7CG",
        "outputId": "dfaedf51-838d-4155-98b5-03b876cc020e"
      },
      "source": [
        "get_example(valid_data, 20)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question : \n",
            "\n",
            "to have a positive impact of the environment\n",
            "\n",
            "Answer : \n",
            "\n",
            "salvage plastic bottles instead of throwing them away\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Th5z-3NV4g"
      },
      "source": [
        "**Build Vocab from the training set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aco_yN_vKtNN"
      },
      "source": [
        "text_field.build_vocab(train_data, min_freq =2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5es-aXDK8eQ",
        "outputId": "022ef83f-e8b6-40ff-8094-491e23af3937"
      },
      "source": [
        "len(text_field.vocab)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3934"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNc5mP9sMlmp"
      },
      "source": [
        "**Both Question and Answers are sharing the same vocab here. We can look at an example to see that. Focus on 'the' token in both examples below**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqatL3knLhLS",
        "outputId": "0ff79aea-26cb-47d2-e951-3e325f290b82"
      },
      "source": [
        "for token in train_data.examples[0].question:\r\n",
        "  print(f\"Token_Text : {token} >>>>> Token_Integer: {text_field.vocab.stoi[token]}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token_Text : the >>>>> Token_Integer: 5\n",
            "Token_Text : sun >>>>> Token_Integer: 77\n",
            "Token_Text : is >>>>> Token_Integer: 7\n",
            "Token_Text : responsible >>>>> Token_Integer: 880\n",
            "Token_Text : for >>>>> Token_Integer: 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZKvtpqRL2ia",
        "outputId": "16986c11-c7a0-49fe-ae43-c24e891a06bd"
      },
      "source": [
        "for token in train_data.examples[1].answer:\r\n",
        "  print(f\"Token_Text : {token} >>>>> Token_Integer: {text_field.vocab.stoi[token]}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token_Text : the >>>>> Token_Integer: 5\n",
            "Token_Text : mountains >>>>> Token_Integer: 516\n",
            "Token_Text : seem >>>>> Token_Integer: 2038\n",
            "Token_Text : smaller >>>>> Token_Integer: 885\n",
            "Token_Text : than >>>>> Token_Integer: 95\n",
            "Token_Text : in >>>>> Token_Integer: 11\n",
            "Token_Text : photographs >>>>> Token_Integer: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNB0qWPrOJMF"
      },
      "source": [
        "**Create iterators for train, valid and test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD-0c_-iOTGO"
      },
      "source": [
        "#Assign device to use\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\r\n",
        "\r\n",
        "#Define Batch Size\r\n",
        "BATCH_SIZE = 128\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "    batch_size = BATCH_SIZE, sort = False,\r\n",
        "    device = device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5oGN3SNO07u"
      },
      "source": [
        "**Let's look at a batch from the training iterator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyXDBQA2L8CI"
      },
      "source": [
        "one_batch = next(iter(train_iterator))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-QtGDKiO-JZ",
        "outputId": "e05119dc-ce2b-4a44-8392-8ec16442f5f4"
      },
      "source": [
        "print(f\"Shape of a batch question: {one_batch.question.shape}\\n\")\r\n",
        "print(f\"Batch Question : {one_batch.question}\\n\")\r\n",
        "print(f\"Shape of a batch answer: {one_batch.answer.shape}\\n\")\r\n",
        "print(f\"Batch Answer : {one_batch.answer}\\n\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of a batch question: torch.Size([36, 128])\n",
            "\n",
            "Batch Question : tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [ 217,  117,   11,  ...,   18,   64,  159],\n",
            "        [ 970,   95,  121,  ...,  275,  246, 1233],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\n",
            "\n",
            "Shape of a batch answer: torch.Size([14, 128])\n",
            "\n",
            "Batch Answer : tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [   0,    0, 2631,  ..., 2741, 3669,   61],\n",
            "        [1759,    3,    3,  ...,    3,    3,    5],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X63r9hLeQpb3"
      },
      "source": [
        "**Now Let's build the Seq2Seq Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZAkHSAuQ_aQ"
      },
      "source": [
        "**ENCODER**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WXvYdSQQ9sC"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(src))\r\n",
        "        \r\n",
        "        #embedded = [src len, batch size, emb dim]\r\n",
        "        \r\n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\r\n",
        "        \r\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\r\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #outputs are always from the top hidden layer\r\n",
        "        \r\n",
        "        return hidden"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRB5riiPRqjW"
      },
      "source": [
        "**DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itn5FT4aRTgD"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.output_dim = output_dim\r\n",
        "        \r\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, input, hidden, context):\r\n",
        "        \r\n",
        "        #input = [batch size]\r\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\r\n",
        "        #context = [n layers * n directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\r\n",
        "        #hidden = [1, batch size, hid dim]\r\n",
        "        #context = [1, batch size, hid dim]\r\n",
        "        \r\n",
        "        input = input.unsqueeze(0)\r\n",
        "        \r\n",
        "        #input = [1, batch size]\r\n",
        "        \r\n",
        "        embedded = self.dropout(self.embedding(input))\r\n",
        "        \r\n",
        "        #embedded = [1, batch size, emb dim]\r\n",
        "                \r\n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\r\n",
        "            \r\n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\r\n",
        "            \r\n",
        "        output, hidden = self.rnn(emb_con, hidden)\r\n",
        "        \r\n",
        "        #output = [seq len, batch size, hid dim * n directions]\r\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\r\n",
        "        \r\n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\r\n",
        "        #output = [1, batch size, hid dim]\r\n",
        "        #hidden = [1, batch size, hid dim]\r\n",
        "        \r\n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \r\n",
        "                           dim = 1)\r\n",
        "        \r\n",
        "        #output = [batch size, emb dim + hid dim * 2]\r\n",
        "        \r\n",
        "        prediction = self.fc_out(output)\r\n",
        "        \r\n",
        "        #prediction = [batch size, output dim]\r\n",
        "        \r\n",
        "        return prediction, hidden\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R__5VlqvRyum"
      },
      "source": [
        "**Seq2Seq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N8ojA3KRu9B"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\r\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\r\n",
        "        \r\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\r\n",
        "        \r\n",
        "        #src = [src len, batch size]\r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\r\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\r\n",
        "        \r\n",
        "        batch_size = trg.shape[1]\r\n",
        "        trg_len = trg.shape[0]\r\n",
        "        trg_vocab_size = self.decoder.output_dim\r\n",
        "        \r\n",
        "        #tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        #last hidden state of the encoder is the context\r\n",
        "        context = self.encoder(src)\r\n",
        "        \r\n",
        "        #context also used as the initial hidden state of the decoder\r\n",
        "        hidden = context\r\n",
        "        \r\n",
        "        #first input to the decoder is the <sos> tokens\r\n",
        "        input = trg[0,:]\r\n",
        "        \r\n",
        "        for t in range(1, trg_len):\r\n",
        "            \r\n",
        "            #insert input token embedding, previous hidden state and the context state\r\n",
        "            #receive output tensor (predictions) and new hidden state\r\n",
        "            output, hidden = self.decoder(input, hidden, context)\r\n",
        "            \r\n",
        "            #place predictions in a tensor holding predictions for each token\r\n",
        "            outputs[t] = output\r\n",
        "            \r\n",
        "            #decide if we are going to use teacher forcing or not\r\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\r\n",
        "            \r\n",
        "            #get the highest predicted token from our predictions\r\n",
        "            top1 = output.argmax(1) \r\n",
        "            \r\n",
        "            #if teacher forcing, use actual next token as next input\r\n",
        "            #if not, use predicted token\r\n",
        "            input = trg[t] if teacher_force else top1\r\n",
        "\r\n",
        "        return outputs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfOXH-4RR3Hi"
      },
      "source": [
        "INPUT_DIM = len(text_field.vocab)\r\n",
        "OUTPUT_DIM = len(text_field.vocab)\r\n",
        "ENC_EMB_DIM = 128\r\n",
        "DEC_EMB_DIM = 128\r\n",
        "HID_DIM = 256\r\n",
        "ENC_DROPOUT = 0.6\r\n",
        "DEC_DROPOUT = 0.6\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\r\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\r\n",
        "\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6vjEEW3R9xn",
        "outputId": "3fc753ae-92bc-4f0b-9b15-642fc18d1bf6"
      },
      "source": [
        "def init_weights(m):\r\n",
        "    for name, param in m.named_parameters():\r\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\r\n",
        "        \r\n",
        "model.apply(init_weights)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(3934, 128)\n",
              "    (rnn): GRU(128, 256)\n",
              "    (dropout): Dropout(p=0.6, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3934, 128)\n",
              "    (rnn): GRU(384, 256)\n",
              "    (fc_out): Linear(in_features=640, out_features=3934, bias=True)\n",
              "    (dropout): Dropout(p=0.6, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DinZgYRLSAW3",
        "outputId": "3e3bdc04-5e53-4311-9122-bd73eb3e92bb"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,318,302 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B351U_HlSETi"
      },
      "source": [
        "optimizer = optim.AdamW(model.parameters())\r\n",
        "PAD_IDX = text_field.vocab.stoi[text_field.pad_token]\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdO6vptUSQZm"
      },
      "source": [
        "**Training Loop and Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF4L_L8eSIlE"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.question\r\n",
        "        trg = batch.answer\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output = model(src, trg)\r\n",
        "        \r\n",
        "        #trg = [trg len, batch size]\r\n",
        "        #output = [trg len, batch size, output dim]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output[1:].view(-1, output_dim)\r\n",
        "        trg = trg[1:].view(-1)\r\n",
        "        \r\n",
        "        #trg = [(trg len - 1) * batch size]\r\n",
        "        #output = [(trg len - 1) * batch size, output dim]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P48nwCEpSaxF"
      },
      "source": [
        "**Evaluation Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2ybDsP9SO0m"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.question\r\n",
        "            trg = batch.answer\r\n",
        "\r\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\r\n",
        "\r\n",
        "            #trg = [trg len, batch size]\r\n",
        "            #output = [trg len, batch size, output dim]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output[1:].view(-1, output_dim)\r\n",
        "            trg = trg[1:].view(-1)\r\n",
        "\r\n",
        "            #trg = [(trg len - 1) * batch size]\r\n",
        "            #output = [(trg len - 1) * batch size, output dim]\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsSnDywaSZ1M"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWNLS7ZWSgd5",
        "outputId": "22c55ccb-6559-484e-a177-5bd7c98dfb36"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'OpenBookQA_model1.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 2s\n",
            "\tTrain Loss: 6.409 | Train PPL: 607.028\n",
            "\t Val. Loss: 5.458 |  Val. PPL: 234.534\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 5.368 | Train PPL: 214.410\n",
            "\t Val. Loss: 5.418 |  Val. PPL: 225.381\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 5.235 | Train PPL: 187.688\n",
            "\t Val. Loss: 5.393 |  Val. PPL: 219.812\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 5.165 | Train PPL: 174.993\n",
            "\t Val. Loss: 5.427 |  Val. PPL: 227.487\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 5.096 | Train PPL: 163.371\n",
            "\t Val. Loss: 5.438 |  Val. PPL: 229.990\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 5.026 | Train PPL: 152.274\n",
            "\t Val. Loss: 5.443 |  Val. PPL: 231.126\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 4.974 | Train PPL: 144.623\n",
            "\t Val. Loss: 5.455 |  Val. PPL: 234.025\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 4.947 | Train PPL: 140.732\n",
            "\t Val. Loss: 5.482 |  Val. PPL: 240.229\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 4.918 | Train PPL: 136.763\n",
            "\t Val. Loss: 5.487 |  Val. PPL: 241.465\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 4.867 | Train PPL: 129.875\n",
            "\t Val. Loss: 5.569 |  Val. PPL: 262.303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wd9sSZ3Sryb",
        "outputId": "d846acec-325c-49d8-b444-6deff7d3ebb4"
      },
      "source": [
        "model.load_state_dict(torch.load('OpenBookQA_model1.pt'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m32ucdDzVPQq",
        "outputId": "dc3d6c55-7ba5-49c5-acf3-bc661d8e0403"
      },
      "source": [
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 5.219 | Test PPL: 184.824 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zykxI0TVVdxd"
      },
      "source": [
        "def show_predictions(question, answer, prediction, example_id):\r\n",
        "  model_prediction = \" \".join([text_field.vocab.itos[x] for x in torch.argmax(prediction[:, example_id, :], axis = 1).cpu()])\r\n",
        "\r\n",
        "  print(\"Question:\\n\")\r\n",
        "  print(\" \".join([text_field.vocab.itos[x] for x in question[:, example_id].cpu()]))\r\n",
        "    \r\n",
        "\r\n",
        "  print(\"\\nActual Answer: \\n\")\r\n",
        "  print(\" \".join([text_field.vocab.itos[x] for x in answer[:, example_id].cpu()]))\r\n",
        "\r\n",
        "  print(\"\\nPredicted Answer: \\n\")\r\n",
        "  print(model_prediction)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zhrlJvCVTLh"
      },
      "source": [
        "example_number = 0\r\n",
        "test_iterator_list = list(test_iterator)\r\n",
        "batch = test_iterator_list[example_number]\r\n",
        "question = batch.question\r\n",
        "answer = batch.answer\r\n",
        "prediction = model(question, answer, 0)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOJbDaerWK0I",
        "outputId": "c44faa5e-6ece-4623-f476-35e63025aa5a"
      },
      "source": [
        "show_predictions(question, answer, prediction, example_number)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:\n",
            "\n",
            "<sos> a person wants to start <unk> money so that they can <unk> a nice <unk> at the end of the year . after looking over their <unk> and <unk> , they decide the best way to save money is to <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Actual Answer: \n",
            "\n",
            "<sos> <unk> eating lunch out <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "\n",
            "Predicted Answer: \n",
            "\n",
            "<unk> <unk> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}